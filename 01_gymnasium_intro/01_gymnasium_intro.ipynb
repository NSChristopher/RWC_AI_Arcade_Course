{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Truth About Becoming a Machine Learning (ML) Engineer\n",
    "\n",
    "Machine learning is increadibly facinating and fun! however a game that you buy at the store will not be designed to be played by a computer (ML algorythm) which brings us to a problem and probably one of the most time intensive parts of ML, feeding the inputs from an environment (virtual or real) into our algorithm.\n",
    "\n",
    "Luckily there is a Python library (a package of code or a software) that already exists and will elimenate this problem for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"gymnasium-text.png\" alt=\"gymnasium\" width=\"400\"/>\n",
    "\n",
    "# ________________________________________________________\n",
    "<p float=\"left\">\n",
    "  <img src=\"adventure.gif\" width=\"98\" />\n",
    "  <img src=\"car_racing.gif\" width=\"225\" /> \n",
    "  <img src=\"frozen_lake.gif\" width=\"150\" />\n",
    "  <img src=\"taxi.gif\" width=\"235\" />\n",
    "</p>\n",
    "\n",
    "Gymnasium, by OpenAI the creators of ChatGPT, will give us access to an endless number of environments, allowing us to test and train our future ML algorythms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "    <img src=\"AE_loop (1).png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gymnasium implements the classic agent environment loop (as seen above) used for renforcement learning. The Agent (or Renforcment Learning Algorythm RL) takes gets an initial observation from the envrionment (their \"State\") then the agent proformes an action (\"Action\") and gets back an observation (\"State\") and reward (\"Reward\"). The reward is given when the Agent reaches a goal or sub goal and it encurages or renforces the learning (hence renforcement learning). The Agent may also get some additional information like whether the game ended or was pre maturally stoped (\"Terminated\" or \"Truncated\").\n",
    "\n",
    "Terms:\n",
    "- Agent: Our renforcement learning algorythm (or us if we are playing the game)\n",
    "- Environment: The virtual world the Agent interacts with\n",
    "- Action: The action taken on the environment\n",
    "- State: The current position of the Agent or what it can currantly observe.\n",
    "- Reward: The reward given for winning the game or proforming a desired action.\n",
    "- Terminated: Game ended by sucess or failure.\n",
    "- Truncated: Game ended unexpectedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initializing Our Environment\n",
    "\n",
    "first we need to import the gymansium package\n",
    "\n",
    "``` python\n",
    "import gymnasium as gym\n",
    "```\n",
    "\n",
    "Then we need to initialize our environment object\n",
    "\n",
    "``` python\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "```\n",
    "\n",
    "This \"env\" object is what we will interact with to collect information about our game environment. gym.make() takes an argument telling gymnasium what game you want to load. in this case it's Cart Pole (a balancing game).\n",
    "\n",
    "### Exercise 1: initialize the Cliff Walking environment\n",
    "\n",
    "Initialize the environment as shown with the Cliff Walking game. See a gif of the environment bellow.\n",
    "\n",
    "<img src=\"cliff_walking.gif\" width=\"400\" />\n",
    "\n",
    "``` python\n",
    "\"CliffWalking-v0\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Getting initial observations\n",
    "\n",
    "Next use the reset() method to get initial observations (or \"State\") from the environment object (\"env\")\n",
    "\n",
    "it will also return info which we will not be using.\n",
    "\n",
    "``` python\n",
    "state, info = env.reset()\n",
    "\n",
    "```\n",
    "\n",
    "### Exercise 2: get and print initial observation\n",
    "\n",
    "in the following code block get the state and print it with the print() method that we used in our python tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice a number (or integer int) was returned. this int corresponds to the position of the character in this environment which happens to be a grid world with each space having a number as shown bellow. Can you see where the character is?\n",
    "\n",
    "|  0  |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  | 10 | 11 |\n",
    "|---|-----|-----|-----|-----|-----|-----|-----|-----|-----|----|----|\n",
    "| 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 20  | 21  | 22 | 23 |\n",
    "| 24  | 25  | 26  | 27  | 28  | 29  | 30  | 31  | 32  | 33  | 34 | 35 |\n",
    "| 36  | 37  | 38  | 39  | 40  | 41  | 42  | 43  | 44  | 45  | 46 | 47 |\n",
    "\n",
    "We can get an image of the current game state by first getting the an rgb collor array and then displaying it with using a simple library called matplot. \n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rgb_array = env.render(mode='rgb_array')\n",
    "\n",
    "plt.imshow(rgb_array)\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Taking a Step in a Specific Direction\n",
    "\n",
    "Now that we have initialized our environment, it's time to start interacting with it. Instead of letting the computer choose random actions, you will be the one deciding which direction to move. Think of yourself as the agent in the game, trying to navigate through the environment.\n",
    "\n",
    "### Actions You Can Take:\n",
    "\n",
    "- `0`: Move left\n",
    "- `1`: Move down\n",
    "- `2`: Move right\n",
    "- `3`: Move up\n",
    "\n",
    "Each action will result in a new observation from the environment, a reward for the action, and an indication of whether the episode (game) is finished.\n",
    "\n",
    "### Example 3: Take a Step to the Right\n",
    "\n",
    "Let's start by taking a step to the right. Here's how you would do it:\n",
    "\n",
    "```python\n",
    "# Define the action as moving right\n",
    "action = 2  # 2 means move right\n",
    "\n",
    "# Take a step in the chosen direction\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Display the state, reward, and if the game is terminated\n",
    "print(f\"State: {state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Done: {terminated}\")\n",
    "\n",
    "#Display the image if you'd like\n",
    "rgb_array = env.render()\n",
    "plt.imshow(rgb_array)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "In this example, youâ€™re moving the agent to the right. The environment will then tell you what it looks like after the move (`observation`), how good or bad the move was (`reward`), whether the game is over (`done`), and any additional information (`info`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you sucessfully moved the character to the right you're probably dead which means you should have gotten a truncated of 1 and be moved to the starting position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get the reward!\n",
    "\n",
    "You now have enough knowledge to navigate the player to the goal and get the reward. see if you can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
